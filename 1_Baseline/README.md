We explored some multimodel LLMs that can handle both vision and language input: Flamingo, LLaVA, MiniGpt-4, BLIP-2, Phi-3.

We have chosen LLaVA for its accuracy and modifiability.

We designed two experiments for our study. The first experiment uses the Philadelphia Naming Test (PNT), which contains 185 relatively simple images of objects. The model is expected to output a single word corresponding to each input image. This experiment will assess the model's word-level understanding.

The second experiment uses discourse task images, which depict more complex scenes involving multiple people and activities. These images are intended to evaluate language abilities. The LLM will generate multi-sentence responses, which can then be assessed for signs of aphasia.
